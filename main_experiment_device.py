# ê³¼ê±° ê°œë°œì¤‘ ì‚¬ìš©í•˜ë˜ ì½”ë“œ. (ìµœì¢… ì‹¤í—˜ ì½”ë“œ ì•„ë‹˜)
# ìµœì¢… í•™ìœ„ë…¼ë¬¸ ì‹¤í—˜ ì½”ë“œëŠ” main_pregenerated.py
#
#


import time
import threading
import json
import random
import csv
from datetime import datetime
import queue
from scheduler.power_aware_topsis_scheduler import PowerAwareTOPSISScheduler
from scheduler.power_only_scheduler import PowerOnlyScheduler
from scheduler.performance_only_scheduler import PerformanceOnlyScheduler
from scheduler.round_robin_scheduler import RoundRobinScheduler
from scheduler.shortest_queue_scheduler import ShortestQueueScheduler
from scheduler.random_scheduler import RandomScheduler
from scheduler.mobility_aware_power_topsis_scheduler import MobilityAwarePowerTOPSISScheduler
from scheduler.base_scheduler import Node
from mobility.mobility_manager import MobilityManager
from task_generator import TaskGenerator
from deadline_adapter import DeadlineAdapter
from prometheus_collector import PrometheusCollector
from inference_request import start_worker_threads, stop_worker_threads, task_queues, WORKERS, format_timestamp, completed_tasks_queue, put_task_to_queue
from slo_monitor import SLOMonitor
from performance_profile import PerformanceProfiler
from utils import *

# â† ì¶”ê°€! Device task generator import
from device_task_generator import (
    start_device_task_generators, 
    stop_device_task_generators, 
    incoming_tasks_queue,
    get_incoming_queue_status,
    save_task_generation_log
)

# task ìƒì„± í›„ ë””ë°”ì´ìŠ¤ í• ë‹¹ (ê¸°ì¡´)
# ê° ë””ë°”ì´ìŠ¤ì—ì„œ task ìƒì„± (ë³€ê²½ ì™„ë£Œ)


def main():
    random.seed(42)

    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜ - Distributed Task Generation ë°©ì‹"""
    print("Starting Power-aware Mobility-based SLO Scheduling Experiment...")
    print("Using Distributed Task Generation Model")
    
    # ì‹¤í—˜ ID ìƒì„±
    experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"Experiment ID: {experiment_id}")
    
    # 1. ì´ˆê¸°í™”
    prometheus_collector = PrometheusCollector("http://localhost:30090")
    slo_monitor = SLOMonitor(experiment_id)
    performance_profiler = PerformanceProfiler()
    
    mobility_manager = MobilityManager(area_size=(500, 500))  # â† ë³€ê²½! ê³µê°„ ì¶•ì†Œ
    task_generator = TaskGenerator(mobility_manager)
    deadline_adapter = DeadlineAdapter(mobility_manager)

    # 2. ë™ì  ì „ë ¥ ì„ê³„ê°’ ê³„ì‚°
    power_threshold = prometheus_collector.calculate_dynamic_power_threshold(list(WORKERS.keys()))
    print(f"Dynamic power threshold: {power_threshold:.1f}W")
    
    # 3. ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
    #scheduler = PowerAwareTOPSISScheduler(
    #    prometheus_collector=prometheus_collector,
    #    task_queues=task_queues, 
    #    power_threshold=power_threshold,
    #    weights=[0.6, 0.3, 0.05, 0.05]
    #)
    #scheduler=PowerOnlyScheduler(prometheus_collector=prometheus_collector)
    #scheduler=PerformanceOnlyScheduler(performance_profiler=performance_profiler)
    #scheduler=RoundRobinScheduler()
    #scheduler=RandomScheduler()
    #scheduler = ShortestQueueScheduler(task_queues=task_queues)

    scheduler = MobilityAwarePowerTOPSISScheduler(
        prometheus_collector=prometheus_collector,
        task_queues=task_queues,
        mobility_manager=mobility_manager,  # â† ì¤‘ìš”! ëª¨ë¹Œë¦¬í‹° ì •ë³´ ì „ë‹¬
        weights=[0.20, 0.50, 0.15, 0.15],
        power_threshold=power_threshold
    )








    # 4. ì›Œì»¤ ë…¸ë“œ ê°ì²´ ìƒì„±
    nodes = []
    for name in WORKERS.keys():
        node = Node(name, WORKERS[name], prometheus_collector)
        nodes.append(node)
        print(f"Node registered: {name} -> {WORKERS[name]}")
    
    print(f"Total {len(nodes)} nodes registered for scheduling")
    
    # 5. ê°€ìƒ IoT ë””ë°”ì´ìŠ¤ ìƒì„± (lambda ìë™ í• ë‹¹ë¨)
    print("Generating virtual IoT devices...")
    task_generator.generate_devices(125, coverage_range=150)  # â† ë³€ê²½! coverage_range ëª…ì‹œ
    
    print(f"Generated 100 devices with lambda rates:")
    lambda_summary = {}
    for device_id, device_info in mobility_manager.devices.items():
        device_type = device_info.device_type
        lambda_summary[device_type] = lambda_summary.get(device_type, 0) + 1
    
    for dtype, count in lambda_summary.items():
        lambda_rate = task_generator.device_lambdas[dtype]
        print(f"  {dtype:12s}: {count:2d} devices Ã— Î»={lambda_rate:.2f} = {count*lambda_rate:.2f} task/sec")
    
    
    # 6. ì´ˆê¸° ì „ë ¥ ìƒíƒœ í™•ì¸
    initial_power = prometheus_collector.get_active_cluster_power(list(WORKERS.keys()))
    print(f"Initial cluster power: {initial_power:.1f}W")
    print(f"Power threshold: {power_threshold:.1f}W")
    
    # 7. ì‹¤í—˜ ì‹œì‘
    start_time = time.time()
    print(f"Experiment started at: {datetime.fromtimestamp(start_time)}")
    
    print("Starting worker threads...")
    worker_threads = start_worker_threads(slo_monitor, performance_profiler, mobility_manager)
    
    # í‰ê°€ì§€í‘œ ìˆ˜ì§‘ìš© ë³€ìˆ˜
    power_samples = [] 

    print("Starting mobility update thread...")
    mobility_thread = threading.Thread(
        target=mobility_update_loop,
        args=(mobility_manager,), 
        daemon=True
    )
    mobility_thread.start()
    
    print("Starting power monitoring thread...")
    power_monitor_thread = threading.Thread(
        target=power_monitor_loop,
        args=(prometheus_collector, list(WORKERS.keys()), power_samples),
        daemon=True
    )
    power_monitor_thread.start()
    
    # Device task generator ìŠ¤ë ˆë“œ ì‹œì‘
    print("Starting device task generator threads...")
    device_threads = start_device_task_generators(
        mobility_manager, 
        task_generator.slo_types, 
        task_generator.slo_ratios
    )
    
    # ë³€ìˆ˜ ì´ˆê¸°í™”
    total_tasks_processed = 0
    total_tasks_skipped = 0
    power_exceeded_count = 0
    no_eligible_node_count = 0
    scheduling_decisions = []
    
    print("="*80)
    print("EXPERIMENT STARTED - Distributed Task Generation Model")
    print("="*80)
    
    # 8. ë©”ì¸ ë£¨í”„ - incoming_tasks_queueì—ì„œ task ì²˜ë¦¬
    # ê¸°ì¡´ forë¬¸ ëŒ€ì‹  queue ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
    
    simulation_duration = 600  # 600ì´ˆ ì‹œë®¬ë ˆì´ì…˜
    target_end_time = start_time + simulation_duration
    
    print(f"Target simulation time: {simulation_duration} seconds")
    print(f"Expected end time: {datetime.fromtimestamp(target_end_time)}")
    
    while time.time() < target_end_time:
        try:
            # incoming_tasks_queueì—ì„œ task êº¼ë‚´ê¸° (1ì´ˆ timeout)
            #priority, sequence, request_timestamp, task = incoming_tasks_queue.get(timeout=1) #ìš°ì„ ìˆœìœ„ í
            request_timestamp, task = incoming_tasks_queue.get(timeout=1)
            



            # IoT ë””ë°”ì´ìŠ¤ mobility ê¸°ë°˜ deadline ë™ì  ì ì‘
            task = deadline_adapter.adapt_deadline(task)
            
            # í˜„ì¬ í´ëŸ¬ìŠ¤í„° ì „ë ¥ ìƒíƒœ í™•ì¸
            current_cluster_power = prometheus_collector.get_active_cluster_power(list(WORKERS.keys()))
            
            # ì „ë ¥ ì„ê³„ê°’ ì´ˆê³¼ ì²´í¬
        #    if current_cluster_power > scheduler.power_threshold:
        #        power_exceeded_count += 1
            


            # ë°ë“œë¼ì¸ ì¤€ìˆ˜ ê°€ëŠ¥í•œ ë…¸ë“œ í•„í„°ë§
            #eligible_nodes = [
            #    node for node in nodes
            #    if performance_profiler.can_meet_deadline(
            #        node.name, task.model_name, task.adapted_deadline
            #    )
            #]
            # ì í•©í•œ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë…¸ë“œ ì¤‘ ì„ íƒ
            #if not eligible_nodes:
            #    eligible_nodes = nodes
            #    no_eligible_node_count += 1
            #    print(f"[WARNING] No node can meet deadline for {task.task_id}")


            #eligible_nodes=nodes # ë…¸ë“œ í•„í„°ë§ x(2025.11.11)
        



            # í ê¸¸ì´ ê¸°ë°˜ í•„í„°ë§
            queue_sizes = {name: task_queues[name].qsize() for name, url in WORKERS.items()}

            max_queue_size = max(queue_sizes.values()) if queue_sizes else 0
            threshold = max_queue_size * 0.7
            
            eligible_nodes = [node for node in nodes if queue_sizes[node.name] <= threshold]
            if not eligible_nodes:
                eligible_nodes = nodes


###            
            # TOPSIS ê¸°ë°˜ ì „ë ¥ ë° SLO ì¸ì‹ ìŠ¤ì¼€ì¤„ë§
            #selected_node = scheduler.schedule(task, eligible_nodes)


            # mobility_aware_schedule
            mobility_info = {'device_id': task.device_id}
            selected_node = scheduler.schedule(task, eligible_nodes, mobility_info=mobility_info)


            task.assigned_node = selected_node.name
            
            # ìŠ¤ì¼€ì¤„ë§ ê²°ì • ê¸°ë¡
            scheduling_decision = {
                'timestamp': request_timestamp,
                'request_timestamp': format_timestamp(request_timestamp),
                'task_id': task.task_id,
                'device_id': task.device_id,
                'slo_type': task.slo_type,
                'base_deadline': task.base_deadline,
                'adapted_deadline': task.adapted_deadline,
                'selected_node': selected_node.name,
                'node_power': selected_node.power_consumption,
                'cluster_power': current_cluster_power,
            }
            scheduling_decisions.append(scheduling_decision)

        
            # ê¸°ë³¸ í---------------
            task_queues[selected_node.name].put((task, request_timestamp))
            
            # ìš°ì„ ìˆœìœ„ í
            #put_task_to_queue(selected_node.name, task, request_timestamp) 



            
            # ì§„í–‰ ìƒí™© ë¡œê·¸ ì¶œë ¥
            total_tasks_processed += 1
            
            if total_tasks_processed % 100 == 0:
                queue_status = get_incoming_queue_status()
                print(f"[Progress] Processed: {total_tasks_processed}, "
                      f"Incoming queue: {queue_status['queue_size']}, "
                      f"Cluster power: {current_cluster_power:.1f}W")
        
        except queue.Empty:
            # Timeout: incoming queueê°€ ë¹„ì–´ìˆìŒ
            # ê³„ì† ëŒ€ê¸°
            pass
    
    print("="*80)
    print("SIMULATION TIME COMPLETED - Waiting for processing to finish...")
    print("="*80)
    
    # â† ì¶”ê°€! Device task generator ìŠ¤ë ˆë“œ ì¢…ë£Œ
    print("Stopping device task generators...")
    stop_device_task_generators()
    
    # Incoming queueì— ë‚¨ì•„ìˆëŠ” task ì²˜ë¦¬
    print("Processing remaining tasks in incoming queue...")
    remaining_count = 0
    while not incoming_tasks_queue.empty():
        try:

            request_timestamp, task = incoming_tasks_queue.get(timeout=0.5) 
            task = deadline_adapter.adapt_deadline(task)
            current_cluster_power = prometheus_collector.get_active_cluster_power(list(WORKERS.keys()))
            
            #eligible_nodes = [
            #    node for node in nodes
            #    if performance_profiler.can_meet_deadline(
            #        node.name, task.model_name, task.adapted_deadline
            #    )
            #]
            
            #if not eligible_nodes:
            #    eligible_nodes = nodes

            
            # í ê¸¸ì´ ê¸°ë°˜ í•„í„°ë§
            queue_sizes = {name: task_queues[name].qsize() for name, url in WORKERS.items()}
            max_queue_size = max(queue_sizes.values()) if queue_sizes else 0
            threshold = max_queue_size * 0.7
            
            eligible_nodes = [node for node in nodes if queue_sizes[node.name] <= threshold]
            if not eligible_nodes:
                eligible_nodes = nodes
            
            if total_tasks_processed % 100 == 0:
                print(f"[FILTER] Q: {queue_sizes} | T: {threshold:.1f} | E: {[n.name for n in eligible_nodes]}")

            selected_node = scheduler.schedule(task, eligible_nodes)
            task.assigned_node = selected_node.name
            
            scheduling_decision = {
                'timestamp': request_timestamp,
                'request_timestamp': format_timestamp(request_timestamp),
                'task_id': task.task_id,
                'device_id': task.device_id,
                'slo_type': task.slo_type,
                'base_deadline': task.base_deadline,
                'adapted_deadline': task.adapted_deadline,
                'selected_node': selected_node.name,
                'node_power': selected_node.power_consumption,
                'cluster_power': current_cluster_power,
            }
            scheduling_decisions.append(scheduling_decision)


            # ê¸°ë³¸ í---------------
            task_queues[selected_node.name].put((task, request_timestamp))
            
            # ìš°ì„ ìˆœìœ„ í
            #put_task_to_queue(selected_node.name, task, request_timestamp) 


            
            remaining_count += 1
            total_tasks_processed += 1
            
        except queue.Empty:
            break
    
    print(f"Processed {remaining_count} remaining tasks from incoming queue")
    
    # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
    for queue_name, q in task_queues.items():
        remaining = q.qsize()
        print(f"Waiting for queue {queue_name} to finish ({remaining} tasks remaining)...")
        q.join()
    
    # ì‹¤í—˜ ì¢…ë£Œ ë° ì •ë¦¬
    end_time = time.time()
    total_duration = end_time - start_time
    
    save_task_generation_log(experiment_id)


    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ
    print("Stopping worker threads...")
    stop_worker_threads(worker_threads)
    
    # í‰ê°€ì§€í‘œ ê³„ì‚°
    violation_stats = slo_monitor.get_violation_stats()
    violations = violation_stats["total"]
    
    total_response_time = 0.0
    total_waiting_time  = 0.0
    completed_count     = 0

    # completed_tasks_queueì—ì„œ ê²°ê³¼ ì½ê¸°
    while not completed_tasks_queue.empty():
        result = completed_tasks_queue.get()
        total_response_time += result["response_time"]
        total_waiting_time  += result["waiting_time"]
        completed_count    += 1

    if len(power_samples) > 1:
        sample_interval = power_samples[1][0] - power_samples[0][0]
    else:
        sample_interval = 1.0

    # í‰ê°€ ë©”íŠ¸ë¦­
    slo_compliance_rate = (completed_count - violations) / completed_count * 100 if completed_count else 0
    throughput = completed_count / total_duration if total_duration > 0 else 0
    avg_response_time = total_response_time / completed_count if completed_count else 0
    avg_waiting_time  = total_waiting_time  / completed_count if completed_count else 0
    total_energy_Wh = sum(power for _, power in power_samples) * sample_interval / 3600
    avg_power_W = sum(power for _, power in power_samples) / len(power_samples) if power_samples else 0
    power_efficiency = completed_count / total_energy_Wh if total_energy_Wh > 0 else 0
    slo_per_energy = slo_compliance_rate / total_energy_Wh if total_energy_Wh > 0 else 0

    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
    save_experiment_results(
        experiment_id, scheduling_decisions, 
        mobility_manager, slo_monitor, 
        start_time, end_time
    )
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    final_cluster_power = prometheus_collector.get_active_cluster_power(list(WORKERS.keys()))
    
    print("="*80)
    print("EXPERIMENT COMPLETED - FINAL RESULTS")
    print("="*80)
    print(f"Experiment ID: {experiment_id}")
    print(f"Total tasks processed: {total_tasks_processed}")
    print(f"Total duration: {total_duration:.2f} seconds ({total_duration/60:.1f} minutes)")
    print(f"Average task arrival rate: {total_tasks_processed/total_duration:.2f} tasks/second")
    print()
    
    print("=== POWER CONSUMPTION ===")
    print(f"Initial cluster power: {initial_power:.1f}W")
    print(f"Final cluster power: {final_cluster_power:.1f}W")
    print(f"Power threshold: {power_threshold:.1f}W")
    print(f"Threshold exceeded: {power_exceeded_count} times")
    print()
    
    print("=== SLO PERFORMANCE ===")
    print(f"Total SLO violations: {violation_stats['total']}")
    if violation_stats['total'] > 0:
        print(f"SLO violation rate: {violation_stats['total']/completed_count*100:.2f}%")
        print(f"Violations by type: {violation_stats['by_type']}")
        print(f"Average violation time: {violation_stats['avg_violation']:.3f}s")
    else:
        print("ğŸ‰ No SLO violations detected!")
    print()
    
    print("=== SCHEDULING PERFORMANCE ===")
    print(f"Tasks with no eligible nodes: {no_eligible_node_count}")
    
    # ë…¸ë“œë³„ ì‘ì—… ë¶„ë°° í†µê³„
    node_task_count = {}
    for decision in scheduling_decisions:
        node = decision['selected_node']
        node_task_count[node] = node_task_count.get(node, 0) + 1
    
    print("Node task distribution:")
    for node, count in sorted(node_task_count.items()):
        percentage = (count / total_tasks_processed) * 100 if total_tasks_processed else 0
        print(f"  {node}: {count} tasks ({percentage:.1f}%)")
    print()
    
    print("=== EVALUATION METRICS ===")
    print(f"SLO Compliance Rate: {slo_compliance_rate:.2f}%")
    print(f"System Throughput: {throughput:.2f} tasks/second")
    print(f"Average Response Time: {avg_response_time:.4f}s")
    print(f"Average Waiting Time: {avg_waiting_time:.4f}s")
    print(f"Completed Count: {completed_count}")
    print(f"Total Energy Consumption: {total_energy_Wh:.3f} Wh")
    print(f"Average Power Consumption: {avg_power_W:.3f} W")
    print(f"Power Efficiency: {power_efficiency:.2f} tasks/Wh")
    print(f"SLO per Energy: {slo_per_energy:.2f} %/Wh")
    print()
    
    print("=== FILES GENERATED ===")
    print(f"Master inference results: master_inference_results.csv")
    print(f"SLO violations: slo_violations_{experiment_id}.csv")
    print(f"Scheduling decisions: scheduling_decisions_{experiment_id}.csv")
    print(f"Device information: devices_{experiment_id}.csv")
    print(f"Experiment summary: experiment_summary_{experiment_id}.json")
    print("="*80)


if __name__ == "__main__":
    main()